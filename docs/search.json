[
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html",
    "href": "statnotes/understanding-gamma/understanding-gamma.html",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "",
    "text": "gamma_params &lt;- function(shape = NULL, scale = 1/rate, rate = 1,\n                         mean = NULL, sd = NULL,\n                         eqs = FALSE){\n  if(eqs){\n    cat(rep(\"=\", 25), \"\\n\")\n    cat(eqs()$gamma, \"\\n\")\n    cat(rep(\"=\", 25), \"\\n\")\n  }else{\n      if(is.null(shape)){\n      var &lt;- sd^2\n      shape &lt;- mean^2 / var\n      scale &lt;- mean / shape\n      rate &lt;- 1/scale\n    } else if(is.null(mean) & is.null(sd)){\n      if(is.null(rate)){\n        scale &lt;- 1/rate\n      } else{\n        rate &lt;- 1/scale\n      }\n      mean &lt;- shape * scale\n      var &lt;- shape * scale^2\n      sd &lt;- sqrt(var)\n    }else{\n      stop(\"when shape and scale are provided, mean and sd need to be NULL (and viceversa)\")\n    }\n    out &lt;- list(shape = shape, scale = scale, rate = rate, mean = mean, var = var, sd = sd)\n    # coefficient of variation\n    out$cv &lt;- 1/sqrt(shape)\n    return(out)\n  }\n}\n\ngammap &lt;- function(mean = NULL,\n                   sd = NULL,\n                   shape = NULL,\n                   scale = NULL,\n                   rate = NULL,\n                   skew = NULL,\n                   cv = NULL){\n    pars &lt;- as.list(environment())\n    spars &lt;- pars[!sapply(pars, is.null)]\n    \n    is_pair &lt;- function(x, pair){\n        all(x %in% pair)\n    }\n    \n    compute &lt;- function(shape, scale) {\n        rate &lt;- 1 / scale\n        mean &lt;- shape * scale\n        sd &lt;- sqrt(shape * scale^2)\n        cv &lt;- 1 / sqrt(shape)\n        skew &lt;- 2 / sqrt(shape)\n        list(shape = shape, rate = rate, scale = scale, cv = cv, skew = skew, mean = mean, sd = sd)\n    }\n    \n    if(is_pair(names(spars), c(\"mean\", \"sd\"))){\n        mean &lt;- spars$mean\n        sd &lt;- spars$sd\n        cv &lt;- sd/mean\n        shape &lt;- 1 / cv^2\n        scale &lt;- mean / shape\n    } else if(is_pair(names(spars), c(\"mean\", \"shape\"))){\n        mean &lt;- spars$mean\n        shape &lt;- spars$shape\n        scale &lt;- mean / shape\n    } else if(is_pair(names(spars), c(\"mean\", \"cv\"))){\n        mean &lt;- spars$mean\n        cv &lt;- spars$cv\n        shape &lt;- 1 / cv^2\n        scale &lt;- mean * cv^2\n    } else if(is_pair(names(spars), c(\"shape\", \"rate\"))){\n        shape &lt;- spars$shape\n        scale &lt;- 1/spars$rate\n    }else if(is_pair(names(spars), c(\"shape\", \"scale\"))){\n        shape &lt;- spars$shape\n        scale &lt;- spars$scale\n    } else{\n        stop(\"combination not implemented!\")\n    }\n    \n    compute(shape, scale)\n    \n}\n\nggamma &lt;- function(shape = NULL,\n                   scale = 1/rate,\n                   rate = 1,\n                   mean = NULL,\n                   sd = NULL,\n                   show = c(\"msd\", \"sr\", \"ss\"),\n                   lf = 5,\n                   size = 15,\n                   ns = 1e4){\n  show &lt;- match.arg(show)\n  argg &lt;- as.list(environment())[1:5]\n  gm &lt;- do.call(gamma_params, argg)\n  if(length(unique(sapply(gm, length))) != 1){\n    stop(\"All vectors need to be of the same length!\")\n  }\n\n  n &lt;- length(gm$mean)\n  range &lt;- c(0, max(gm$mean) + lf * max(gm$sd))\n  x &lt;- seq(range[1], range[2], length.out = ns)\n  d &lt;- mapply(function(sh, sc) dgamma(x, shape = sh, scale = sc), gm$shape, gm$scale, SIMPLIFY = FALSE)\n  D &lt;- data.frame(x = rep(x, n),\n                  d = unlist(d),\n                  shape = rep(gm$shape, each = length(x)),\n                  scale = rep(gm$scale, each = length(x)),\n                  rate = rep(gm$rate, each = length(x)),\n                  mean = rep(gm$mean, each = length(x)),\n                  sd = rep(gm$sd, each = length(x))\n  )\n\n  if(show == \"msd\"){\n    D$cond &lt;- factor(sprintf(\"$\\\\mu = %.3f$, $\\\\sigma = %.3f$\", D$mean, D$sd))\n  }else if(show == \"ss\"){\n    D$cond &lt;- factor(sprintf(\"$k (shape) = %.3f$, $\\\\theta (scale) = %.3f$\", D$shape, D$scale))\n  }else{\n    D$cond &lt;- factor(sprintf(\"$\\\\alpha (shape) = %.3f$, $\\\\beta (rate) = %.3f$\", D$shape, D$rate))\n  }\n\n  ggplot(D, aes(x = x, y = d, color = cond)) +\n    geom_line(lwd = 1) +\n    scale_color_discrete(labels = lapply(levels(D$cond), latex2exp::TeX)) +\n    theme_minimal(15) +\n    theme(legend.title = element_blank(),\n          legend.position = c(.95, .95),\n          legend.justification = c(\"right\", \"top\"),\n          legend.box.just = \"right\",\n          legend.margin = margin(6, 6, 6, 6)\n    ) +\n    ylab(latex2exp::TeX(\"dgamma($x$, $\\\\alpha$/$k$, $\\\\theta$, $\\\\beta$)\"))\n}"
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#poisson-distribution",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#poisson-distribution",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe poisson distribution PMF (probability mass function) is represented in Equation 1.\n\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\tag{1}\\]\n\nlambda &lt;- 10\nx &lt;- seq(0, 30, 1)\n\nggplot(data = data.frame(x)) +\n    geom_segment(aes(x = x, xend = x, y = 0, yend = dpois(x, lambda))) +\n    ggtitle(tex(\"\\\\lambda = 10\"))\n\n\n\n\n\n\n\n\nThe mean if the poisson is just \\(\\lambda\\) and the variance is still \\(\\lambda\\) (thus the standard deviaton is \\(\\sqrt{\\lambda}\\)). This is also very intuitive because as \\(\\lambda\\) decrease, the spread of the distribution decrease because there it is left-bounded on zero. This is a common feature of Exponential, Gamma and Poisson distributions.\nAn assumption of the Poisson distribution is that \\(\\lambda\\) is fixed for every observation and this is the most common limitation that often (if not always) lead to overdispersion, but this is another story.\nAs a practical example, let’s assume that we are observing the number of cars passing on a certain street during 1 hour. 1 hour is our unit time and \\(\\lambda\\) is the average number of cars i.e. the average rate of cars."
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#exponential-distribution",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#exponential-distribution",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nThe exponential distribution is a continous probability distribution that describe the time taken for the first event of a Poisson process to happen.\n\\[\nf(x; \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n\\]\nThe only parameter of the exponential distribution is, as for the Poisson distribution, the \\(\\lambda\\).\nUsing the cars example, an exponential distribution with \\(\\lambda = 10\\) means that for a unit time we expect on average that a car will take \\(1/\\lambda = 0.1\\) and with a rate of \\(\\lambda = 10\\) events per unit time.\n\nggcurve(fun = dexp, args = list(rate = 10)) +\n    geom_vline(xintercept = 1/10)\n\n\n\n\n\n\n\n\nIn fact, the mean of the exponential distribution is \\(1/\\lambda\\) because is like computing the unit time divided by the expected number of events. Unit time is 1 and the expected number of events is the rate (\\(\\lambda\\)) of the Poisson."
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#gamma-distribution",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#gamma-distribution",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nTo sum up, the Poisson distribution describe a discrete process counting the number of independent events for a unit time with rate \\(\\lambda\\). The Exponential distribution describe the average time taken for the first event to happen given the rate \\(\\lambda\\).\nWhat about if we have more than 1 event? When \\(k &gt; 1\\) essentially we have a Gamma distribution.\n\\[\nf(x; \\alpha, \\lambda) = \\frac{x^{\\alpha-1} e^{-\\lambda x} \\lambda^{\\alpha}}{\\Gamma(\\alpha)}, \\quad x &gt; 0, \\quad \\alpha, \\lambda &gt; 0\n\\]\nIn this equation, \\(\\alpha = k\\) i.e. the usually called shape parameter is the number of events. If we fix \\(\\alpha = 1\\) the Gamma distribution reduces to an Exponential distribution. In practice the Exponential is a special type of Gamma with \\(\\alpha = 1\\).\n\nggcurve(fun = dgamma,\n        args = list(shape = 1, rate = 10))\n\n\n\n\n\n\n\n\nSticking with our example, a Gamma distribution with rate \\(\\lambda = 10\\) and shape \\(\\alpha = 5\\) is about the time taken for \\(k = 5\\) events to happen given the rate of 10 events per unit time.\n\nggcurve(fun = dgamma,\n        args = list(shape = 5, rate = 10),\n        xlim = c(0, 3))\n\n\n\n\n\n\n\n\nWe will describe better the parametrization but if the time taken for 1 events to happen given \\(\\lambda = 10\\) is \\(1/\\lambda = 0.1\\). The time taken for \\(k = 5\\) events is just \\(\\frac{1}{\\lambda} k = \\frac{k}{\\lambda}\\).\n\nggcurve(fun = dgamma,\n        args = list(shape = 5, rate = 10),\n        xlim = c(0, 3)) +\n        geom_vline(xintercept = 1/10 * 5)\n\n\n\n\n\n\n\n\nSo the Gamma is just the sum of \\(k\\) independent exponential distributions.\n\nClearly, we can model whatever process respect the assumptions and not only time. But this is the natural intepretation given the link between Poisson, Exponential and Gamma.\n\nThe problem is that when we have situation not directly related to time and poisson processes is less intuitive to think as in previous examples. For this reason we can use different type of Gamma parametrization. This is the main reason why the Gamma is so confusing because there are multiple parametrizations.\n\nShape \\(\\alpha\\) and Rate \\(\\lambda\\) parametrization\nThis is the parametrization that we discussed above. The properties of the Gamma distribution using this approach are:\n\nmean: \\(\\mu = \\frac{\\alpha}{\\lambda}\\)\nvariance: \\(\\sigma^2 = \\frac{\\alpha}{\\lambda^2}\\) (similar to the Poisson)\nstandard deviation: \\(\\sigma = \\frac{\\sqrt{\\alpha}}{\\lambda}\\)\nskewness: \\(\\frac{2}{\\sqrt{\\alpha}}\\)\n\nNotice some important aspects:\n\nas for the Poisson, mean and standard deviations are linked. When \\(\\mu = \\frac{\\alpha}{\\lambda}\\) increase also \\(\\sigma = \\frac{\\sqrt{\\alpha}}{\\lambda}\\) increase\nthe relationship between mean and standard deviation is linear for the Poisson but not linear for the Gamma.\n\n\n\nShape \\(\\alpha\\) and Scale \\(\\theta\\) parametrization\nThis is similar to the parametrizatio above but the scale parameter is defined as the reciprocal of the rate parameter thus \\(\\theta = 1/\\lambda\\).\n\nmean: \\(\\mu = \\alpha\\theta\\)\nvariance: \\(\\sigma^2 = \\alpha\\theta^2\\)\nstandard deviation: \\(\\sigma = \\sqrt{\\alpha}\\theta\\)\nskewness: \\(\\frac{2}{\\sqrt{\\alpha}}\\)\n\n\n\n\\(\\mu\\) and shape \\(\\alpha\\) parametrization\nThis is probably the most useful at least for modelling. In fact, in R thge default when dealing with the Gamma distribution per-se (dgamma, rgamma, etc.) is using the two parametrization above. While when fitting a GLM with a Gamma distribution the glm (but also glmer) function use the \\(\\mu\\) and shape \\(\\alpha\\) parametrization.\nWith this parametrization we need to do a little bit of math for fixing the mean and the shape and then finding the scale or rate to generate data using base R function.\nBut how to fix the shape? We are somehow losing the event/time logic of the beginning because we simply want a Gamma with a certain mean. A good way to think about the shape is in terms of skewness. The skewness depends only on the \\(\\alpha\\) parameter thus we can choose our Gamma fixing the mean and the desired level of skewness. Solving the equation for \\(\\alpha\\) we have \\(\\alpha = 4/s^2\\) (s is skewness). Then having \\(\\mu\\) and \\(\\alpha\\) we can calculate \\(\\theta\\) or \\(\\lambda\\). For example, using \\(\\lambda\\), \\(\\lambda = \\mu / \\alpha\\).\n\nmu &lt;- 100\nskew &lt;- 1\nalpha &lt;- 4/skew^2\nlambda &lt;- alpha / mu\n# theta &lt;- mu / alpha\n\ny &lt;- rgamma(1e4, shape = alpha, rate = lambda)\nhist(y, breaks = 50)\n\n\n\n\n\n\n\nmean(y)\n## [1] 99.4626\nsd(y)\n## [1] 49.24929\npsych::skew(y)\n## [1] 0.9619901\n\nLet’s fit a GLM:\n\nfit &lt;- glm(y ~ 1, family = Gamma(link = \"log\"))\nsummary(fit)\n\n\nCall:\nglm(formula = y ~ 1, family = Gamma(link = \"log\"))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.599782   0.004952     929   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.2451774)\n\n    Null deviance: 2557.8  on 9999  degrees of freedom\nResidual deviance: 2557.8  on 9999  degrees of freedom\nAIC: 104612\n\nNumber of Fisher Scoring iterations: 4\n\n\nGiven that we used the log link function, \\(e^{\\beta_0}\\) is \\(\\mu\\) and \\(\\alpha = 1/\\zeta\\) where \\(\\zeta\\) is the estimated dispersion parameter.\n\nexp(coef(fit)) # mean\n## (Intercept) \n##     99.4626\nzeta &lt;- summary(fit)$dispersion\n1/zeta # alpha, shape\n## [1] 4.07868\n\nAn important point that is not always clear is that the assumption of a standard glm model fitted with glm or glmer is that the shape parameter is the same for each observation. This is similar to a standard model where the residual variance is the same for each observation.\nThis can be relaxed using a so-called scale-location model (e.g., using brms) with predictors on the mean (as glm) and the shape parameters.\n\n\n\\(\\mu\\) and \\(\\sigma^2\\) parametrization\nIn this case, we fix the \\(\\mu\\) and \\(\\sigma^2\\) but we have no control on the skewness.\n\nggamma(mean = c(500, 600), sd = c(200, 200))\n\n\n\n\n\n\n\n\n\ngammap(mean = 500, sd = 200)\n\n$shape\n[1] 6.25\n\n$rate\n[1] 0.0125\n\n$scale\n[1] 80\n\n$cv\n[1] 0.4\n\n$skew\n[1] 0.8\n\n$mean\n[1] 500\n\n$sd\n[1] 200\n\n\nIn addition, let’s assume we want to simulate two groups with different mean and same variance as the plot above:\n\ngammap(mean = c(500, 600), sd = c(200, 200))\n\n$shape\n[1] 6.25 9.00\n\n$rate\n[1] 0.0125 0.0150\n\n$scale\n[1] 80.00000 66.66667\n\n$cv\n[1] 0.4000000 0.3333333\n\n$skew\n[1] 0.8000000 0.6666667\n\n$mean\n[1] 500 600\n\n$sd\n[1] 200 200\n\n\nThe shape is different between the two groups, and this is a violation of the assumption of the standard GLM similary to having different residual variances between conditions.\nIn fact, when we model this dataset with a GLM, the shape is not recovered:\n\npars &lt;- gammap(mean = c(500, 600), sd = c(200, 200))\nn &lt;- 1e3\ng0 &lt;- rgamma(n, shape = pars$shape[1], scale = pars$scale[1])\ng1 &lt;- rgamma(n, shape = pars$shape[2], scale = pars$scale[2])\n\ny &lt;- c(g0, g1)\nx &lt;- rep(0:1, each = n)\n\nfit &lt;- glm(y ~ x, family = Gamma(link = \"log\"))\nsummary(fit)\n\n\nCall:\nglm(formula = y ~ x, family = Gamma(link = \"log\"))\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.22003    0.01152  539.77   &lt;2e-16 ***\nx            0.16309    0.01630   10.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.13279)\n\n    Null deviance: 285.59  on 1999  degrees of freedom\nResidual deviance: 272.30  on 1998  degrees of freedom\nAIC: 26673\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe mean shift is correctly recovered:\n\n# ~ mean g0\nexp(coef(fit)[1])\n## (Intercept) \n##    502.7173\n\n# ~ mean g1\nexp(coef(fit)[1] + coef(fit)[2])\n## (Intercept) \n##    591.7712\n\n# ~ ratio\nexp(coef(fit)[2]) # 600/500 = 1.2\n##        x \n## 1.177145\n\nBut the dispersion is not recovered:\n\nzeta &lt;- summary(fit)$dispersion\n1/zeta # alpha, shape\n## [1] 7.530687\nmean(pars$shape) # ~ maybe\n## [1] 7.625"
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#different-packages",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#different-packages",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Different packages",
    "text": "Different packages\n\nbrms\nbrms use a mean-shape parametrization, especially when using a location-scale (i.e., distributional) model.\n\nlibrary(brms)\n\nset.seed(2025)\n\npp &lt;- gammap(mean = 100, sd =  50)\npp\n\n$shape\n[1] 4\n\n$rate\n[1] 0.04\n\n$scale\n[1] 25\n\n$cv\n[1] 0.5\n\n$skew\n[1] 1\n\n$mean\n[1] 100\n\n$sd\n[1] 50\n\ny &lt;- rgamma(100, shape = pp$shape, scale = pp$scale)\ndat &lt;- data.frame(y = y)\nfit_brm &lt;- brm(y ~ 1, family = Gamma(link = \"log\"), data = dat, file = \"fit_brm.rds\")\n\n\nsummary(fit_brm)\n\n Family: gamma \n  Links: mu = log; shape = identity \nFormula: y ~ 1 \n   Data: dat (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.59      0.05     4.49     4.69 1.00     3416     2586\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     3.95      0.53     2.97     5.06 1.00     3387     2733\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYou can use also another link for the shape (usually log)\n\n\nglm/glmer\nglm or glmer use the \\(\\mu\\)-dispersion (\\(\\phi\\)) parametrization where dispersion is \\(1/\\alpha\\). Using the same dataset:\n\nfit_glm &lt;- glm(y ~ 1, data = dat, family = Gamma(link = \"log\"))\nsummary(fit_glm)\n\n\nCall:\nglm(formula = y ~ 1, family = Gamma(link = \"log\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.58679    0.04903   93.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.2404142)\n\n    Null deviance: 26.262  on 99  degrees of freedom\nResidual deviance: 26.262  on 99  degrees of freedom\nAIC: 1049.5\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(fit_glm)$dispersion\n\n[1] 0.2404142\n\n1/summary(fit_glm)$dispersion\n\n[1] 4.159487\n\n\n\n\ngamlss\ngamlss is a very complete package for scale-location modelling. The package use another parametrization. Basically what in the package is called sigma is \\(\\sqrt{\\phi}\\):\n\nlibrary(gamlss)\n\nfit_gam &lt;- gamlss(y ~ 1, data = dat, family = GA(mu.link = \"log\", sigma.link = \"identity\")) # sigma.link can be also \"log\"\n\nGAMLSS-RS iteration 1: Global Deviance = 1045.458 \nGAMLSS-RS iteration 2: Global Deviance = 1045.458 \n\nsummary(fit_gam)\n\n******************************************************************\nFamily:  c(\"GA\", \"Gamma\") \n\nCall:  gamlss(formula = y ~ 1, family = GA(mu.link = \"log\",  \n    sigma.link = \"identity\"), data = dat) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  log\nMu Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.58679    0.05021   91.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  identity\nSigma Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.50209    0.03412   14.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  100 \nDegrees of Freedom for the fit:  2\n      Residual Deg. of Freedom:  98 \n                      at cycle:  2 \n \nGlobal Deviance:     1045.458 \n            AIC:     1049.458 \n            SBC:     1054.668 \n******************************************************************\n\nsqrt(fit_gam$sigma.coefficients) # same as summary(fit_glm)$dispersion\n\n(Intercept) \n  0.7085856 \n\n\nThe variance of the Gamma is \\(\\sigma^2 = \\phi^2 \\mu^2\\) (see ?GA()):\n\nsd(dat$y)\n\n[1] 48.139\n\n# sqrt(dispersion)\nsqrt(fit_gam$sigma.coefficients^2 * exp(coef(fit_gam))^2)\n\n(Intercept) \n   49.29486"
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#note-about-the-link-function",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#note-about-the-link-function",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Note about the link function",
    "text": "Note about the link function\nThe canonical link function for the Gamma is the inverse, while the most common is the log. By default, glm use the inverse. This change the parameters interpretation.\nThe default link for the dispersion is the identity (e.g., in brms) especially when there are no predictors on the shape or dispersion. With predictors, the usual link function is the log to avoid negative values."
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#mean-variance-relationship",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#mean-variance-relationship",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Mean-Variance relationship1",
    "text": "Mean-Variance relationship1\nAn important point is the mean-variance relationship that is common to all GLMs. For example in the poisson \\(E[y] = \\lambda\\) and \\(V[y] = \\lambda\\).\nIn the Gamma distribution, for the shape-scale parametrization:\n\\[\nE[y] = \\mu = \\alpha\\theta\n\\]\n\\[\nV[y] = \\sigma^2 = \\alpha\\theta^2 = \\mu\\theta\n\\]\nThus if \\(\\mu\\) is 100, the variance is \\(100\\theta\\)\nFor the shape-rate parametrization:\n\\[\nE[y] = \\mu = \\frac{\\alpha}{\\lambda}\n\\]\n\\[\nV[y] = \\sigma^2 = \\frac{\\alpha}{\\lambda^2} = \\mu \\frac{1}{\\lambda} = \\frac{\\mu}{\\lambda}\n\\]\nIt is very strange but the coefficient of variation defined as \\(\\sigma/\\mu\\) depends only on \\(\\alpha\\) (stick with the shape-scale parametrization):\n\\[\nCV = \\frac{\\sigma^2}{\\mu} = \\frac{\\sqrt{\\alpha\\beta^2}}{\\alpha\\beta} = \\frac{1}{\\alpha}\n\\]\n\\[\n\\sigma = \\mu CV \\\\\n\\sigma = \\mu \\frac{1}{\\sqrt{\\alpha}} = \\frac{\\mu}{\\sqrt{\\alpha}}\n\\]\nSo if the mean of the Gamma is 100, the standard deviation is \\(\\frac{100}{\\sqrt{\\alpha}}\\)\nEssentially, the relationship between the mean and the variance (or standard deviation) is adjustable compared to other probability distributions such as the Poisson."
  },
  {
    "objectID": "statnotes/understanding-gamma/understanding-gamma.html#footnotes",
    "href": "statnotes/understanding-gamma/understanding-gamma.html#footnotes",
    "title": "Understanding (hopefully) the Gamma distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfrom https://civil.colorado.edu/~balajir/CVEN6833/lectures/c-czado-GLM-lectures/GammaGLM-01.pdf↩︎"
  },
  {
    "objectID": "statnotes/sensitivity-analysis/index.html",
    "href": "statnotes/sensitivity-analysis/index.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "# Packages\n\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(BayesFactor)\n\n# Seed for simulation\n\nset.seed(2021)\n\n\nGeneral idea\nThe sensitivity analysis is a way to estimate the effect size that a given experiment can reach with a certain sample size, desired power and alpha level [@perugini2018practical].\nThe power analysis is usually considered a procedure that estimate a single number (i.e., the sample size) required for a given statistical analysis to reach a certain power level. Is better to consider the power level as a function with fixed and free parameters.\nIn the case of the a priori power analysis, we fix the power level (e.g., \\(1 - \\beta = 0.80\\)) the alpha level (e.g., \\(\\alpha = 0.05\\)“) and the hypothetical effect size (e.g, \\(d = 0.3\\)). Then we simulate or derive analytically the minimum sample size required for reaching the target power level, given the effect size.\nA more appropriate approach is to consider the sample size a free parameter and calculate the power level for a range of sample size, obtaining the power curve. This is very easy using the pwr package. We assume:\n\n\\(1-\\beta = 0.8\\)\n\\(\\alpha = 0.05\\)\n\\(d = 0.3\\)\nan independent sample t-test situation\n\n\npower_analysis &lt;- pwr::pwr.t.test(\n  d = 0.3, \n  power = 0.8,\n  sig.level = 0.05\n)\npower_analysis\n\n\n     Two-sample t test power calculation \n\n              n = 175.3847\n              d = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nFrom the output we need 175.3846666 subjects per group for reaching the desired power level. As said before a better approach is analyzing the entire power curve. We can simply plot the power_analysis object:\n\nplot(power_analysis)\n\n\n\n\n\n\n\n\n\n\nPower by simulation\nThe previous example is based on the analytically power computation that is possible for simple statistical test. A more general approach is the power analysis by simulation.\nIf we know the statistical assumptions of our analysis we can simulate data accordingly several times (e.g., 10000 simulations) and simply count the number of p values below the alpha level. This is a little bit too much for a simple t-test but can be really insightful.\nWe need to simulate two groups sampled from two populations with different mean (our effect size) and with the same standard deviation. We can simulate directly on the cohen's d scale setting the standard deviation to 1 and the mean difference to the desired d level.\nFor obtaining the power curve we need a range of sample size from 10 to 200 for example.\nThere are multiple ways to approach a simulation. Here I declare my parameters and create a grid of values using the tidyr::expand_grid() function to create all combinations. The I simply need to loop for each row, generate data using rnorm, calculate the t-test and then count how many p-values are below the alpha level.\n\nmp0 &lt;- 0\nmp1 &lt;- 0.3\nsd_p &lt;- 1\n# d = mp1 - mp0 / sigma = (0.3 - 0) / 1 = 0.3\nsample_size &lt;- seq(10, 200, 30)\nnsim &lt;- 1000\nalpha_level &lt;- 0.05\n\nsim &lt;- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 &lt;- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 &lt;- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] &lt;- t.test(g0, g1)$p.value\n}\n\nsim %&gt;% \n  group_by(sample_size, mp1) %&gt;% \n  summarise(power = mean(p_value &lt; alpha_level)) %&gt;% \n  ggplot(aes(x = sample_size, y = power)) +\n  geom_point(size = 3) +\n  geom_line() +\n  ggtitle(paste(\"Effect size = \", sim$mp1[1]))\n\n\n\n\n\n\n\n\nThe result is very similar to the pwr result. Increasing the number of simulation will stabilize the results. As said before, using this approach for a t-test is not convenient but with the same code and idea we can simulate an unequal variance situation or having different sample size per group.\n\n\nSensitivity Analysis\nUsing the same approach as before, we can perform a sensivity analysis simply changing our free parameters in the previous simulation. The sensitivity analysis is usually performed with a given sample size and the the free parameter will be the effect size. We can use a range from 0 (the null effect) to 1 and fixing a sample size of 50 subjects per group.\n\nmp0 &lt;- 0\nmp1 &lt;- seq(0, 1, 0.2)\nsd_p &lt;- 1\nsample_size &lt;- 50\nnsim &lt;- 1000\nalpha_level &lt;- 0.05\n\nsim &lt;- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 &lt;- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 &lt;- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] &lt;- t.test(g0, g1)$p.value\n}\n\nsim %&gt;% \n  group_by(mp1, sample_size) %&gt;% \n  summarise(power = mean(p_value &lt; alpha_level)) %&gt;% \n  ggplot(aes(x = mp1, y = power)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", size = 1, col = \"red\") +\n  ggtitle(paste(\"Sample size = \", sim$sample_size[1]))\n\n\n\n\n\n\n\n\nWith the simulation approach we simply have to change our grid of values and calculate the power grouping for effect size instead of sample size. Here we understand that with a sample size of 50 we can detect with 80% power an effect size of ~0.6. If the true effect size is lower than the maximum detectable effect size, we are using an under-powered design.\n\n\nScript\n\n## -----------------------------------------------------------------------------\n## Script: Sensitivity analysis\n##\n## Author: Filippo Gambarota\n## -----------------------------------------------------------------------------\n\n# Packages ----------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(furrr)\n\n# Environment -------------------------------------------------------------\n\nset.seed(2021)\n\n# Functions ---------------------------------------------------------------\n\n# Find the closest target from a vector\n\nfind_closest_n &lt;- function(vector, target){\n  index &lt;- which.min(abs(vector - target))\n  out &lt;- vector[index]\n  return(out)\n}\n\n# Return the minimun effect size given a sample size and the power level\n\nmin_effect &lt;- function(data, sample_size, power_level){\n  ns &lt;- find_closest_n(unique(data$sample_size), sample_size)\n  min(data$effect_size[data$sample_size == ns & data$power &gt;= power_level])\n}\n\n# Calculate power\n\ncompute_power &lt;- function(data, alpha){\n  data %&gt;% \n    group_by(sample_size, effect_size) %&gt;% \n    summarise(power = mean(ifelse(p_value &lt; alpha, 1, 0)))\n}\n\n# Plot the contour\n\npower_contour &lt;- function(data){\n  data %&gt;% \n    ggplot(aes(x = sample_size, y = effect_size, z = power)) +\n    geom_contour_filled(breaks = seq(0,1,0.1)) +\n    coord_cartesian() +\n    cowplot::theme_minimal_grid()\n}\n\n# Plot the power curve\n\npower_curve &lt;- function(data, n){\n  ns &lt;- find_closest_n(unique(data$sample_size), n)\n  data %&gt;% \n    filter(sample_size == ns) %&gt;% \n    ggplot(aes(x = effect_size, y = power)) +\n    geom_point() +\n    geom_line() +\n    cowplot::theme_minimal_grid() +\n    ggtitle(paste(\"Sample size =\", ns))\n}\n\n# Setup simulation --------------------------------------------------------\n\nsample_size &lt;- seq(10, 500, 50)\neffect_size &lt;- seq(0, 1, 0.1)\nnsim &lt;- 1000\n\nsim &lt;- expand_grid(\n  sample_size,\n  effect_size,\n  sim = 1:nsim\n)\n\n# Test --------------------------------------------------------------------\n\nplan(multisession(workers = 4))\n\nsim$p_value &lt;- furrr::future_map2_dbl(sim$sample_size, sim$effect_size, function(x, y){\n  g0 &lt;- rnorm(x, 0, 1)\n  g1 &lt;- rnorm(x, y, 1)\n  t.test(g0, g1)$p.value\n}, .options = furrr_options(seed = TRUE))\n\n# Computing power\n\nsim_power &lt;- compute_power(sim, alpha = 0.05)\n\n# Plots -------------------------------------------------------------------\n\n# Contour plot\n\npower_contour(sim_power)\n\n# Power curve\n\npower_curve(sim_power, 200)\n\n\n\nBayesian\n\nsim &lt;- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0,\n  bf = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 &lt;- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 &lt;- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] &lt;- t.test(g0, g1)$p.value\n  sim[i, \"bf\"] &lt;- extractBF(ttestBF(g0,g1))$bf\n}\n\nsim %&gt;% \n  group_by(mp1, sample_size) %&gt;% \n  summarise(power = mean(p_value &lt; alpha_level),\n            bf = mean(log(bf))) %&gt;% \n  pivot_longer(c(power, bf), names_to = \"metric\", values_to = \".value\") %&gt;% \n  ggplot(aes(x = mp1, y = .value)) +\n  facet_wrap(~metric, scales = \"free\") +\n  geom_point(size = 3) +\n  geom_line() +\n  ggtitle(paste(\"Sample size = \", sim$sample_size[1]))\n\n\n\n\n\n\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.2.3 (2023-03-15)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BayesFactor_0.9.12-4.7 Matrix_1.5-3           coda_0.19-4           \n [4] pwr_1.3-0              lubridate_1.9.2        forcats_1.0.0         \n [7] stringr_1.5.0          dplyr_1.1.2            purrr_1.0.1           \n[10] readr_2.1.4            tidyr_1.3.0            tibble_3.2.1          \n[13] ggplot2_3.5.0          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10        compiler_4.2.3     pillar_1.9.0       tools_4.2.3       \n [5] digest_0.6.31      lattice_0.20-45    timechange_0.2.0   jsonlite_1.8.4    \n [9] evaluate_0.20      lifecycle_1.0.3    gtable_0.3.3       pkgconfig_2.0.3   \n[13] rlang_1.1.0        cli_3.6.1          rstudioapi_0.14    parallel_4.2.3    \n[17] yaml_2.3.7         mvtnorm_1.1-3      xfun_0.39          fastmap_1.1.1     \n[21] withr_2.5.0        knitr_1.42         MatrixModels_0.5-1 generics_0.1.3    \n[25] vctrs_0.6.2        htmlwidgets_1.6.2  hms_1.1.3          grid_4.2.3        \n[29] tidyselect_1.2.0   glue_1.6.2         R6_2.5.1           pbapply_1.7-0     \n[33] fansi_1.0.4        rmarkdown_2.21     farver_2.1.1       tzdb_0.4.0        \n[37] magrittr_2.0.3     codetools_0.2-19   scales_1.3.0       htmltools_0.5.5   \n[41] colorspace_2.1-0   labeling_0.4.2     utf8_1.2.3         stringi_1.7.12    \n[45] munsell_0.5.0     \n\n\n\n\nReferences"
  },
  {
    "objectID": "statnotes/machine-learning/index.html",
    "href": "statnotes/machine-learning/index.html",
    "title": "Machine Learning Notes",
    "section": "",
    "text": "library(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:caret':\n\n    lift"
  },
  {
    "objectID": "statnotes/machine-learning/index.html#custom-loo-cv-function",
    "href": "statnotes/machine-learning/index.html#custom-loo-cv-function",
    "title": "Machine Learning Notes",
    "section": "Custom LOO-CV function",
    "text": "Custom LOO-CV function\nThis function implement a not elegant function for testing the loo-cv. Is useful for understanding the idea:\n\nmy_loocv &lt;- function(fit){\n  \n  dat &lt;- fit$model\n  y &lt;- all.vars(fit$call)[1]\n  \n  errors &lt;- vector(mode = \"numeric\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i &lt;- dat[i, ]\n    fit_no_i &lt;- update(fit, data = dat[-i, ])\n    pred_i &lt;- predict(fit_no_i, to_pred_i)\n    errors[i] &lt;- pred_i - to_pred_i[1, y]\n  }\n  return(errors)\n}\n\nComparing with the function from caret, this is the OLS model:\n\ncaret_ols &lt;- train(mpg ~ disp + wt, \n               data = mtcars, \n               trControl=trainControl(method=\"LOOCV\"), \n               method=\"lm\")\n\nMy custom function:\n\nstandard_ols &lt;- lm(mpg ~ disp + wt, data = mtcars)\nmy_loocv &lt;- get_rmse(my_loocv(standard_ols))\n\nThe output is exactly the same:\n\ncaret_ols$results[\"RMSE\"] == my_loocv\n\n  RMSE\n1 TRUE"
  },
  {
    "objectID": "statnotes/machine-learning/index.html#loo-cv-and-model-complexity",
    "href": "statnotes/machine-learning/index.html#loo-cv-and-model-complexity",
    "title": "Machine Learning Notes",
    "section": "LOO-CV and model complexity",
    "text": "LOO-CV and model complexity\n\npredictors &lt;- colnames(mtcars) # getting all predictors\npredictors &lt;- predictors[-(predictors == \"mpg\")] # keeping only Xs\n\nfit_list &lt;- vector(mode = \"list\", length = length(predictors))\n\nfit_i &lt;- lm(mpg ~ 1, data = mtcars)\n\nfor(i in 1:length(predictors)){\n  fit_i &lt;- update(fit_i, formula(paste(\". ~ . +\", predictors[i])))\n  fit_list[[i]] &lt;- fit_i\n}\n\nComputing the actual LOO-CV:\n\nmy_loocv &lt;- function(fit){\n  \n  dat &lt;- fit$model\n  y &lt;- all.vars(fit$call)[1]\n  \n  errors &lt;- vector(mode = \"numeric\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i &lt;- dat[i, ]\n    fit_no_i &lt;- update(fit, data = dat[-i, ])\n    pred_i &lt;- predict(fit_no_i, to_pred_i)\n    errors[i] &lt;- pred_i - to_pred_i[1, y]\n  }\n  return(errors)\n}\n\ncv &lt;- map_dbl(fit_list, function(x) get_rmse(my_loocv(x))) # get loo-cv mean error\nnpred &lt;- map_dbl(fit_list, function(i) length(all.vars(i$call))-2) # get number of predictors\nr2 &lt;- map_dbl(fit_list, function(mod) summary(mod)$r.squared) # get rsquared from fitted models\n\nloo_cv &lt;- data.frame(\n  cv, r2, npred\n)\n\n# Plotting\n\nloo_cv %&gt;% \n  tidyr::pivot_longer(c(1,2), names_to = \"measure\", values_to = \"value\") %&gt;% \n  ggplot(aes(x = npred, y = value)) +\n  geom_line() +\n  geom_point(size = 3) +\n  facet_wrap(~measure, scales = \"free\") +\n  cowplot::theme_minimal_grid()"
  },
  {
    "objectID": "statnotes/machine-learning/index.html#loo-cv-and-lasso-regression",
    "href": "statnotes/machine-learning/index.html#loo-cv-and-lasso-regression",
    "title": "Machine Learning Notes",
    "section": "LOO-CV and Lasso regression",
    "text": "LOO-CV and Lasso regression\n\ngrid &lt;- 10^seq(1, -2, length = 100) # grid of lambda values\nx &lt;- model.matrix(mpg ~ ., mtcars)[, -1] # predictors\ny &lt;- mtcars$mpg # response variable\n\nFitting the lasso regression:\n\nfit_lasso &lt;- glmnet(x, y, alpha = 1, lambda = grid)\n\nCustom function for computing the lasso and loo-cv:\n\nmy_loocv_lasso &lt;- function(dat, fit){\n  \n  errors &lt;- vector(mode = \"list\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i &lt;- x[i, ]\n    fit_no_i &lt;- glmnet(x[-i, ], y[-i], alpha = 1, lambda = grid)\n    pred_i &lt;- predict(fit_no_i, newx = t(to_pred_i))\n    errors[[i]] &lt;- pred_i - y[i]\n  }\n  return(errors)\n} \n\nget_min &lt;- function(target, to_minimize){\n  target[which.min(to_minimize)]\n}\n\nComputing the loo-cv:\n\nerrors_lasso &lt;- my_loocv_lasso(mtcars, fit_lasso) # loo-cv\n\ncv_lasso &lt;- do.call(rbind, errors_lasso) # combining lists\n\nmse_lasso &lt;- apply(cv_lasso, 2, function(x) mean(x^2)) # computing error\n\nPlotting the \\(\\lambda\\) value as a function of the mean-squared error:\n\nplot(grid, mse_lasso)\n\n\n\n\n\n\n\n\nThe minimum error is associated with the 0.7564633."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "This repository is a collections of my slides and talks.\n\n\n\nIntroduction to the Tidyverse - Psicostat 11/12/2020 - PDF\nCreating an Academic Website with Hugo - HTML - Repository\nHow to conduct a bibliographic research (Italian) - HTML\n\n\n\n\n\nIntroduction to Meta-analysis - PDF\nTips on Power and Sensitivity analysis - HTML\nMultivariate Meta-Analysis in the Multiverse - Psicostat 04/03/2022 - HTML - PDF\nSimulating meta-analysis - Psicostat Workshops 03/05/2023 - HTML\nReplicating or not? Statistical methods for replicability assessment - Psicostat 27/10/2023 - HTML\n\n\n\n\n\nIntroduction to the Neural Correlate of Consciousness - [Mechanism of Consciousness and Error Monitoring Course - University of Padova] - Slides\nIntroduction to the Scientific Study of Consciousness - [General Psychology Course - University of Padova] - Slides [Italian]\n\n\n\n\n\nVisual Working Memory and Face Processing - [Cognitive and Functional Bases of Intersubjectivity - University of Padova] - Slides"
  },
  {
    "objectID": "slides.html#methodological-slides",
    "href": "slides.html#methodological-slides",
    "title": "Slides",
    "section": "",
    "text": "Introduction to the Tidyverse - Psicostat 11/12/2020 - PDF\nCreating an Academic Website with Hugo - HTML - Repository\nHow to conduct a bibliographic research (Italian) - HTML"
  },
  {
    "objectID": "slides.html#statistics",
    "href": "slides.html#statistics",
    "title": "Slides",
    "section": "",
    "text": "Introduction to Meta-analysis - PDF\nTips on Power and Sensitivity analysis - HTML\nMultivariate Meta-Analysis in the Multiverse - Psicostat 04/03/2022 - HTML - PDF\nSimulating meta-analysis - Psicostat Workshops 03/05/2023 - HTML\nReplicating or not? Statistical methods for replicability assessment - Psicostat 27/10/2023 - HTML"
  },
  {
    "objectID": "slides.html#consciousness",
    "href": "slides.html#consciousness",
    "title": "Slides",
    "section": "",
    "text": "Introduction to the Neural Correlate of Consciousness - [Mechanism of Consciousness and Error Monitoring Course - University of Padova] - Slides\nIntroduction to the Scientific Study of Consciousness - [General Psychology Course - University of Padova] - Slides [Italian]"
  },
  {
    "objectID": "slides.html#working-memory",
    "href": "slides.html#working-memory",
    "title": "Slides",
    "section": "",
    "text": "Visual Working Memory and Face Processing - [Cognitive and Functional Bases of Intersubjectivity - University of Padova] - Slides"
  },
  {
    "objectID": "slides.html#unconscious-visual-working-memory-a-critical-review-and-bayesian-meta-analysis",
    "href": "slides.html#unconscious-visual-working-memory-a-critical-review-and-bayesian-meta-analysis",
    "title": "Slides",
    "section": "Unconscious Visual Working Memory: a critical review and Bayesian meta-analysis",
    "text": "Unconscious Visual Working Memory: a critical review and Bayesian meta-analysis\n\n3 minute talk - Talk"
  },
  {
    "objectID": "slides.html#trackdown-package",
    "href": "slides.html#trackdown-package",
    "title": "Slides",
    "section": "Trackdown Package",
    "text": "Trackdown Package\n\ntrackdown package - TASK4-2021 - Slides\ntrackdown package - useR-2021 - Slides - Talk\ntrackdown package - SIPS-2021 - Slides - Talk"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "",
    "section": "",
    "text": "I’m a (very beginner 😁) coding keen! I love everything related to code and I’m trying to keep learning everyday!"
  },
  {
    "objectID": "coding.html#coding-projects",
    "href": "coding.html#coding-projects",
    "title": "",
    "section": "Coding Projects",
    "text": "Coding Projects\n\ntodoget: a python script for tracking specific tags (#TODO, #TOREVIEW) within a generic folder\ntrackdown: a R package to collaborate on RMarkdown documents using Google Docs"
  },
  {
    "objectID": "coding.html#languages",
    "href": "coding.html#languages",
    "title": "",
    "section": "Languages",
    "text": "Languages\n\n is my main language! I’m very fluent with R and I teach also university level courses (check the ARCA courses).\nI’m learning Python especially for creating psychological experiments and automatizing stuff on linux\nI use Matlab for doing image processing, analyzing EEG data and creating psychological experiments\nI use HTML and CSS mainly for producing documents, slides, websites and reports together with RMarkdown\nI use bash for managing my Linux laptop (I’ve recently switched to linux and I totally love that penguin )"
  },
  {
    "objectID": "coding.html#markup",
    "href": "coding.html#markup",
    "title": "",
    "section": "Markup",
    "text": "Markup\n\nI totally love Markdown! I use it for everything (slides, reports, websites, notes, etc.)\nI know a little bit of Latex, especially with the RMarkdown and knitr framework"
  },
  {
    "objectID": "coding.html#what-i-want-to-learnimprove",
    "href": "coding.html#what-i-want-to-learnimprove",
    "title": "",
    "section": "What I want to learn/improve",
    "text": "What I want to learn/improve\n\nPython! especially with data science oriented modules\nThe HTML/CSS/Javascript combination"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Academic",
    "section": "",
    "text": "Academic\n\n stat-teaching for my teaching materials\n shared-research for my papers\n students-starting for templates for students\n filippogambarota for my projects\n\n\n\nWorkplace\n\nI work at the Department of Developmental Psychology and Socialization, University of Padova.\nMy office is 022, 5th floor CLA building, Via Venezia, 16, 35131 Padova (PD)\n\n\n\nContact\n\nYou can write me at  filippo.gambarota@unipd.it\nZoom: https://unipd.zoom.us/my/filippo.gambarota\n\n\n\nSocial Media\nI’m trying to declutter social media (without a great success Tbh 😵) however you can find me on:\n\nTwitter \nInstagram \nI’m a TV-Shows addicted 😵! Check my Trakt profile\n\n \n\nTweets by fgambarota"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Filippo Gambarota",
    "section": "",
    "text": "I am a post-doc researcher at the University of Padova, Department of Developmental Psychology and Socialisation.\nMy research concern meta-analysis, data simulation for power and design analysis and multiverse analysis. I am part of the Psicostat research group.\nIn my free time I love coding and I am a speaker coach for TEDx Padova."
  },
  {
    "objectID": "statnotes.html",
    "href": "statnotes.html",
    "title": "",
    "section": "",
    "text": "Warning in readLines(qmd): riga finale incompleta in\n'statnotes/understanding-gamma/understanding-gamma.qmd'\n\nMachine Learning Notes [Last update: 2025-01-27]\nAggregating or not? [Last update: 2025-01-27]\nSensitivity Analysis [Last update: 2025-01-27]\nSimulating Effect Sizes [Last update: 2025-01-27]\nUnderstanding (hopefully) the Gamma distribution [Last update: 2025-01-27]\nUnderstanding interactions [Last update: 2025-01-27]"
  },
  {
    "objectID": "statnotes/meta-analysis-aggregating-or-not/index.html",
    "href": "statnotes/meta-analysis-aggregating-or-not/index.html",
    "title": "Aggregating or not?",
    "section": "",
    "text": "With complex data structures in meta-analysis sometimes we want to aggregate effect size to simplify the analysis. As reported by James Pustejovsky, “sometimes, aggregating effect sizes is fine”. The blog post show formally why aggregating or not, in specific condition, brings the exactly same results with some additional pros in terms of computational speed. Crucially, this is true only in specific conditions.\n\nIn this post I’ll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to exactly the same results as a multivariate model. This occurs when two conditions are met: - We are not interested in within-study heterogeneity of effects - Any predictors included in the model vary between studies but not within a given study (i.e., effect sizes from the same study all have the same values of the predictors). In short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level.\n\nThe example refers to a situation where I have multiple effects collected on the same pool of subjects nested within papers. Thus if I have predictors at the level of the effect or I’m interested in modelling variability within papers, aggregating is not good.\nIn case where I do not have moderators or I’m interested only in between-papers variability, aggregating is totally fine.\nWe can expand this even to situations with more nesting level, outcomes nested within experiments nested within papers. If we are not interested in the effect sizes level (i.e., outcome level) aggregating is fine."
  },
  {
    "objectID": "statnotes/meta-analysis-aggregating-or-not/index.html#model-without-aggregation",
    "href": "statnotes/meta-analysis-aggregating-or-not/index.html#model-without-aggregation",
    "title": "Aggregating or not?",
    "section": "Model without aggregation",
    "text": "Model without aggregation\nTo compare the model with and without aggregation, the trick is fitting a model without the lowest nesting level in the random effect structure. Normally, the simulated dataset above should be modeled (for the random part) as 1|paper/exp/effect. Here we drop the effect term:\n\nfit0 &lt;- rma.mv(yi, V, random = ~1|paper/exp, data = dat, sparse = TRUE)\n\nsummary(fit0)\n\n\nMultivariate Meta-Analysis Model (k = 293; method: REML)\n\n   logLik   Deviance        AIC        BIC       AICc   \n-463.2780   926.5559   932.5559   943.5862   932.6393   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed     factor \nsigma^2.1  0.3506  0.5921    100     no      paper \nsigma^2.2  0.1651  0.4063    193     no  paper/exp \n\nTest for Heterogeneity:\nQ(df = 292) = 2486.0170, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval    ci.lb   ci.ub    \n  0.0010  0.0699  0.0147  0.9883  -0.1360  0.1381    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUsing the V matrix we are fixing the correlation between multiple outcomes (i.e., the lowest nesting level) to 0.7."
  },
  {
    "objectID": "statnotes/meta-analysis-aggregating-or-not/index.html#model-with-aggregation",
    "href": "statnotes/meta-analysis-aggregating-or-not/index.html#model-with-aggregation",
    "title": "Aggregating or not?",
    "section": "Model with aggregation",
    "text": "Model with aggregation\nTo aggregate we use the Borenstein method implemented in the aggregate function:\n\n# with aggregation\ndat &lt;- escalc(yi = yi, vi = vi, data = dat)\ndatl &lt;- split(dat, dat$paper)\ndatl &lt;- lapply(datl, function(x) aggregate(x, cluster = exp, rho = 0.7))\ndatagg &lt;- do.call(rbind, datl)\n\nhead(datagg)\n\n\n    paper exp effect        b0_i       b0_ij      b0_ijz     vi x      yi \n1       1   1    1.5 -0.04382947 -0.23146442 -0.06469504 0.0565 1 -0.2700 \n2.1     2   1    1.5  0.15294661  0.09919803  0.23345811 0.0578 1  0.5742 \n2.2     2   2    1.0  0.15294661  0.32233407  0.20481287 0.0975 1  1.2265 \n2.3     2   3    1.0  0.15294661  0.15360518  0.04071347 0.0607 1  0.6342 \n3       3   1    1.0 -0.00408931  0.23980867 -0.20986505 0.0874 1  0.0607 \n4.1     4   1    1.5  0.25987891 -0.17661214  0.19485017 0.0701 1  0.4567 \n\n\nThen we fit the same model but without using V (sampling errors are never correlated now) instead we use v and the same random structure:\n\nfit1 &lt;- rma.mv(yi, vi, random = ~1|paper/exp, data = datagg, sparse = TRUE)\n\nsummary(fit1)\n\n\nMultivariate Meta-Analysis Model (k = 193; method: REML)\n\n   logLik   Deviance        AIC        BIC       AICc   \n-196.5763   393.1527   399.1527   408.9251   399.2803   \n\nVariance Components:\n\n            estim    sqrt  nlvls  fixed     factor \nsigma^2.1  0.3506  0.5921    100     no      paper \nsigma^2.2  0.1651  0.4063    193     no  paper/exp \n\nTest for Heterogeneity:\nQ(df = 192) = 1825.2162, p-val &lt; .0001\n\nModel Results:\n\nestimate      se    zval    pval    ci.lb   ci.ub    \n  0.0010  0.0699  0.0147  0.9883  -0.1360  0.1381    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe two models are exactly the same."
  },
  {
    "objectID": "statnotes/simulating-effect-sizes/simulating-effect-sizes.html",
    "href": "statnotes/simulating-effect-sizes/simulating-effect-sizes.html",
    "title": "Simulating Effect Sizes",
    "section": "",
    "text": "Intro\nThe paper by Viechtbauer (2005) provide a clear way to simulate standardized (SMD) and unstandardized (UMD) effect sizes for simulation studies.\n\n\nUnstandardized effect sizes\nWe define two independent groups as:\n\\[\nX^C_{ij} \\sim N(\\mu^C_i, \\sigma^2_i) \\\\\nX^E_{ij} \\sim N(\\mu^E_i, \\sigma^2_i)\n\\]\nand the UMD as:\n\\[\nES_i = \\mu^E_i - \\mu^C_i\n\\]\nWith sampling variance:\n\\[\n\\sigma^2_{\\epsilon_i} = s^2_i\\left(\\frac{1}{n^E_i} + \\frac{1}{n^C_i}\\right)\n\\]\nWhere \\(s^2_i\\) is the typical pooled within-group variance.\nIn this case we simulate participant-level data:\n\nES &lt;- 0.3\ns &lt;- 1\nn_E &lt;- 30\nn_C &lt;- 30\nk &lt;- 30\n\ndi &lt;- rnorm(k, ES, s * (1/n_E + 1/n_C))"
  },
  {
    "objectID": "statnotes/understanding-interactions/index.html",
    "href": "statnotes/understanding-interactions/index.html",
    "title": "Understanding interactions",
    "section": "",
    "text": "x1 &lt;- c(\"a\", \"b\")\nx2 &lt;- c(\"c\", \"d\")\n\ndat &lt;- expand.grid(id = 1:20, x1 = x1, x2 = x2)\ncontrasts(dat$x1) &lt;- c(-0.5, 0.5)\ncontrasts(dat$x2) &lt;- c(-0.5, 0.5)\ndat$y &lt;- rnorm(nrow(dat))\n\n# grand mean (intercept)\n\ngm &lt;- mean(aggregate(y ~ x1 + x2, data = dat, mean)$y)\n\n# main effect x1\n\nmx1 &lt;- diff(aggregate(y ~ x1, data = dat, mean)$y)\n\n# main effect x2\n\nmx2 &lt;- diff(aggregate(y ~ x2, data = dat, mean)$y)\n\n# plot\n\ninteraction.plot(dat$x1, dat$x2, dat$y, fun = mean)\n\n\n\n\n\n\n\n# interaction x1:x2\n\nint &lt;- aggregate(y ~ x1 + x2, data = dat, mean)\nint &lt;- tidyr::pivot_wider(int, names_from = c(x1, x2), values_from = y)\nintx1x2 &lt;- (int$a_c - int$a_d) - (int$b_c  - int$b_d)\n\n# model\n\nfit &lt;- lm(y ~ x1 * x2, data = dat)\n\ncar::Anova(fit, type = \"3\")\n\nAnova Table (Type III tests)\n\nResponse: y\n            Sum Sq Df F value Pr(&gt;F)\n(Intercept)  0.011  1  0.0119 0.9134\nx1           0.117  1  0.1207 0.7292\nx2           0.064  1  0.0660 0.7980\nx1:x2        0.046  1  0.0478 0.8275\nResiduals   73.364 76               \n\nrbind(\"model\" = coef(fit),\n      \"manual\" = c(gm, mx1, mx2, intx1x2))\n\n       (Intercept)         x11         x21    x11:x21\nmodel   0.01197932 -0.07633766 -0.05642767 0.09607497\nmanual  0.01197932 -0.07633766 -0.05642767 0.09607497"
  }
]